# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding를 공부하고 BERT 구조를 구현하였습니다. 

# 환경
* google colab

# train data
* 청와대 국민청원

# train loss

| Batch  | average loss |
| ------------- | ------------- |
| 0  |11.16948127746582|
| 500  |10.430378525556918|
| 1000  |10.158831773580728|
| 1500  |10.043376201474612|
| 2000  |9.982355090155117|
| 2500  |9.943701237309032|

* 추가적인 학습 가능하지만 resource가 부족하여 멈췄음

# reference
https://arxiv.org/pdf/1810.04805.pdf
